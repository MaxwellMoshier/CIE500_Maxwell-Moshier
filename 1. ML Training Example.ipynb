{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic Neural Network with PyTorch â€” Iris Dataset\n",
        "\n",
        "1. Load and inspect data  \n",
        "2. Train / validation / test split  \n",
        "3. Data preprocessing (scaling)  \n",
        "4. Build a basic neural network (MLP)  \n",
        "5. Training loop  \n",
        "6. Testing and evaluation  \n",
        "\n",
        "The Iris dataset is fully numeric and clean, so preprocessing focuses on **scaling and splitting**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# sklearn\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_iris\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load and Inspect the Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df[\"label\"] = y\n",
        "\n",
        "df.head(), df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Train / Validation / Test Split\n",
        "\n",
        "We split first to avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(X_train))\n",
        "print(\"Val size:  \", len(X_val))\n",
        "print(\"Test size: \", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Data Preprocessing: Feature Scaling\n",
        "\n",
        "Neural networks are sensitive to feature scales.\n",
        "We **fit the scaler on training data only**, then apply it to validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled   = scaler.transform(X_val)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "X_train_scaled.mean(axis=0), X_train_scaled.std(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Convert to PyTorch Tensors and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_val_t   = torch.tensor(X_val_scaled,   dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test_scaled,  dtype=torch.float32)\n",
        "\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
        "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t),\n",
        "                          batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(TensorDataset(X_val_t, y_val_t),\n",
        "                          batch_size=32, shuffle=False)\n",
        "test_loader  = DataLoader(TensorDataset(X_test_t, y_test_t),\n",
        "                          batch_size=32, shuffle=False)\n",
        "\n",
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Define a Basic Neural Network (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = MLP(input_dim=4, hidden_dim=32, num_classes=3)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Training Loop (with Validation Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_accuracy(loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return correct / total\n",
        "\n",
        "def train_one_epoch(loader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "n_epochs = 100\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss = train_one_epoch(train_loader)\n",
        "    val_acc = evaluate_accuracy(val_loader)\n",
        "    if epoch % 20 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Final Test Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict(loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        y_true.append(yb.numpy())\n",
        "        y_pred.append(pred.cpu().numpy())\n",
        "    return np.concatenate(y_true), np.concatenate(y_pred)\n",
        "\n",
        "y_true, y_pred = predict(test_loader)\n",
        "\n",
        "print(\"Test accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Classification report:\", classification_report(y_true, y_pred, target_names=target_names))\n",
        "print(\"Confusion matrix:\", confusion_matrix(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "CIE500",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
